import os
import shutil
import hashlib
import re
import threading
import stat
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from core.config import SUPPORTED_EXTENSIONS, REPO_DOWNLOAD_DIR, VECTOR_STORE_DIR
from core.factory import get_embedding_model

try:
    from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
except ImportError:
    from streamlit.scriptrunner import add_script_run_ctx, get_script_run_ctx


def get_repo_id(repo_url):
    # åŒ…å« #branch åœ¨å…§çš„å®Œæ•´å­—ä¸²åš Hashï¼Œé€™æ¨£ä¸åŒåˆ†æ”¯æœƒå­˜æˆä¸åŒ ID
    return hashlib.md5(repo_url.strip().rstrip("/").encode()).hexdigest()


def clean_url(url):
    """
    æ·¨åŒ– URLï¼Œæ”¯æ´ HTTPS å’Œ SSH æ ¼å¼ã€‚
    åŒæ™‚ä¿ç•™ #branch è³‡è¨Šä»¥ä¾¿å¾ŒçºŒè§£æžã€‚
    """
    # é€™è£¡çš„ Regex æœƒæŠ“å–ç›´åˆ°ç©ºç™½ç‚ºæ­¢çš„å­—ä¸²ï¼ŒåŒ…å« #
    match = re.search(r'((?:https?://|git@)[^\s]+)', url)
    if match: return match.group(1)
    return url.strip()


def force_remove_readonly(func, path, excinfo):
    try:
        os.chmod(path, stat.S_IWRITE)
        func(path)
    except Exception as e:
        print(f"âš ï¸ å¼·åˆ¶åˆªé™¤å¤±æ•—: {path}, éŒ¯èª¤: {e}")


def remove_repo_data(repo_url):
    repo_id = get_repo_id(repo_url)
    repo_path = os.path.join(REPO_DOWNLOAD_DIR, repo_id)
    db_path = os.path.join(VECTOR_STORE_DIR, repo_id)
    deleted = False
    if os.path.exists(repo_path): shutil.rmtree(repo_path, onerror=force_remove_readonly); deleted = True
    if os.path.exists(db_path): shutil.rmtree(db_path, onerror=force_remove_readonly); deleted = True
    return deleted


def ingest_repo(repo_url, progress_callback=None, force_update=False, embedding_config=None):
    import git
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_core.documents import Document

    try:
        main_thread_ctx = get_script_run_ctx()
    except Exception:
        main_thread_ctx = None

    class CloneProgress(git.RemoteProgress):
        def update(self, op_code, cur_count, max_count=None, message=''):
            if main_thread_ctx: add_script_run_ctx(threading.current_thread(), main_thread_ctx)
            operation = "Cloning"
            if op_code & git.RemoteProgress.COUNTING:
                operation = "Counting"
            elif op_code & git.RemoteProgress.COMPRESSING:
                operation = "Compressing"
            elif op_code & git.RemoteProgress.RECEIVING:
                operation = "Receiving"
            elif op_code & git.RemoteProgress.RESOLVING:
                operation = "Resolving"
            if max_count:
                percent = int(cur_count / max_count * 100)
                ui_progress = 10 + int(percent * 0.3)
                msg = f"â¬‡ï¸ {operation}: {percent}% ({cur_count}/{max_count})"
            else:
                ui_progress = 10
                msg = f"â¬‡ï¸ {operation}: {cur_count} objects..."
            if progress_callback: progress_callback(msg, ui_progress)

    # 1. è™•ç† URL èˆ‡ Branch
    repo_url = clean_url(repo_url)

    # è§£æž #branch èªžæ³•
    target_branch = None
    if "#" in repo_url:
        repo_url, target_branch = repo_url.rsplit("#", 1)
        print(f"ðŸ“ åµæ¸¬åˆ°æŒ‡å®šåˆ†æ”¯: {target_branch}")

    # 2. è¨ˆç®— ID (ä½¿ç”¨åŒ…å«åˆ†æ”¯çš„åŽŸå§‹ URL æ¦‚å¿µï¼Œä½†é€™è£¡ç‚ºäº†æ–¹ä¾¿é‡æ–°çµ„è£å­—ä¸²å‚³çµ¦ get_repo_id)
    # æ³¨æ„ï¼šæˆ‘å€‘å·²ç¶“åœ¨å¤–é¢å‚³é€²ä¾†çš„æ™‚å€™æ±ºå®šäº† repo_url (å« #)ï¼Œæ‰€ä»¥ get_repo_id æœƒç®—å‡ºå”¯ä¸€çš„ ID
    # é€™è£¡æˆ‘å€‘éœ€è¦ç”¨ "åŽŸå§‹çš„å®Œæ•´è¼¸å…¥" ä¾†ç®— IDï¼Œç¢ºä¿ä¸åŒåˆ†æ”¯åˆ†é–‹å­˜
    full_url_for_id = f"{repo_url}#{target_branch}" if target_branch else repo_url
    repo_id = get_repo_id(full_url_for_id)

    repo_path = os.path.join(REPO_DOWNLOAD_DIR, repo_id)
    db_path = os.path.join(VECTOR_STORE_DIR, repo_id)
    hash_file = os.path.join(VECTOR_STORE_DIR, repo_id, "commit_hash.txt")

    def update_status(msg, progress):
        if progress_callback: progress_callback(msg, progress)

    # 0. æª¢æŸ¥ Hash (ls-remote ä¹Ÿæ”¯æ´åˆ†æ”¯)
    update_status("ðŸ” æª¢æŸ¥ç‰ˆæœ¬...", 5)
    g = git.cmd.Git()
    try:
        g.config("--global", "http.postBuffer", "524288000")
        # å¦‚æžœæœ‰æŒ‡å®šåˆ†æ”¯ï¼Œls-remote éœ€è¦æŒ‡å®š ref
        ref = target_branch if target_branch else 'HEAD'
        latest_hash = g.ls_remote(repo_url, ref).split('\t')[0]
    except Exception:
        latest_hash = None

    if not force_update and os.path.exists(db_path) and os.path.exists(hash_file):
        with open(hash_file, "r") as f:
            local_hash = f.read().strip()
        if latest_hash and local_hash == latest_hash:
            update_status("âš¡ ç‰ˆæœ¬æœªè®Šï¼Œè¼‰å…¥å¿«å–", 100)
            return db_path, "skipped"

    if os.path.exists(repo_path): shutil.rmtree(repo_path, onerror=force_remove_readonly)
    if os.path.exists(db_path): shutil.rmtree(db_path, onerror=force_remove_readonly)

    # 1. Clone (æ”¯æ´åˆ†æ”¯åƒæ•¸)
    update_status(f"â¬‡ï¸ é–‹å§‹ Clone ({target_branch if target_branch else 'Default'})...", 10)
    try:
        clone_kwargs = {
            "depth": 1,
            "single_branch": True,
            "progress": CloneProgress()
        }
        if target_branch:
            clone_kwargs["branch"] = target_branch

        git.Repo.clone_from(repo_url, repo_path, **clone_kwargs)

    except Exception as e:
        error_msg = str(e)
        if "exit code(128)" in error_msg:
            if "not found" in error_msg.lower():
                raise Exception(f"æ‰¾ä¸åˆ°å°ˆæ¡ˆ: {repo_url}")
            elif "permission denied" in error_msg.lower() or "publickey" in error_msg.lower():
                raise Exception(f"SSH æ¬Šé™æ‹’çµ•ã€‚è«‹ç¢ºèª SSH Key è¨­å®šã€‚\nç¶²å€: {repo_url}")
            else:
                raise Exception(f"Git Clone å¤±æ•—: {error_msg}")
        raise Exception(f"Clone å¤±æ•—: {e}")

    update_status("ðŸ“‚ æŽƒææª”æ¡ˆä¸­...", 45)
    raw_documents = []
    MAX_FILES = 5000
    file_count = 0

    for root, dirs, files in os.walk(repo_path):
        if file_count >= MAX_FILES: break
        for file in files:
            ext = os.path.splitext(file)[1].lower()
            if ext in SUPPORTED_EXTENSIONS:
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, repo_path)
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                    if not content.strip(): continue

                    doc = Document(
                        page_content=content,
                        metadata={"source": rel_path, "repo": repo_url}  # é€™è£¡ repo åªå­˜ URLï¼Œä¸ä¸€å®šè¦å­˜ branch
                    )
                    raw_documents.append(doc)
                    file_count += 1
                    if file_count >= MAX_FILES: break
                except:
                    pass
        if file_count % 200 == 0: update_status(f"ðŸ“‚ å·²è®€å– {file_count} å€‹æª”æ¡ˆ...",
                                                45 + int(file_count / MAX_FILES * 15))

    if not raw_documents: raise Exception("No valid files found.")

    update_status(f"âœ‚ï¸ åˆ‡åˆ†èˆ‡æ³¨å…¥ä¸Šä¸‹æ–‡...", 60)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150
    )

    split_docs = text_splitter.split_documents(raw_documents)

    final_docs = []
    for doc in split_docs:
        rel_path = doc.metadata.get("source", "unknown")
        doc.page_content = f"File: {rel_path}\nRepo: {repo_url}\n\n{doc.page_content}"
        final_docs.append(doc)

    total_chunks = len(final_docs)

    if not embedding_config:
        embedding_config = {"provider": "Ollama", "model": "nomic-embed-text", "base_url": "http://localhost:11434"}

    update_status(f"ðŸ§  åˆå§‹åŒ–å‘é‡è¨ˆç®— ({embedding_config['provider']})...", 65)

    embedding_model = get_embedding_model(
        provider=embedding_config['provider'],
        model_name=embedding_config['model'],
        api_key=embedding_config.get('api_key'),
        base_url=embedding_config.get('base_url')
    )

    if not embedding_model: raise Exception("Embedding æ¨¡åž‹åˆå§‹åŒ–å¤±æ•—")

    db = Chroma(embedding_function=embedding_model, persist_directory=db_path)

    BATCH_SIZE = 64
    MAX_WORKERS = 12

    def compute_batch_embeddings(batch_docs):
        for attempt in range(3):
            try:
                b_texts = [d.page_content for d in batch_docs]
                b_embeddings = embedding_model.embed_documents(b_texts)
                return batch_docs, b_embeddings
            except Exception as e:
                if attempt == 2: raise e
                time.sleep(1 * (attempt + 1))

    total_processed = 0
    futures = []
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        update_status(f"ðŸš€ å…¨é€Ÿé‹ç®—ä¸­... (Chunks={total_chunks})", 68)

        for i in range(0, total_chunks, BATCH_SIZE):
            batch = final_docs[i: i + BATCH_SIZE]
            futures.append(executor.submit(compute_batch_embeddings, batch))

        completed_batches = 0
        for future in as_completed(futures):
            try:
                batch_docs, batch_embeddings = future.result()
                db.add_texts(texts=[d.page_content for d in batch_docs], metadatas=[d.metadata for d in batch_docs],
                             embeddings=batch_embeddings)

                total_processed += len(batch_docs)
                completed_batches += 1

                elapsed_time = time.time() - start_time
                if completed_batches > 0:
                    avg_time_per_batch = elapsed_time / completed_batches
                    remaining_batches = len(futures) - completed_batches
                    eta_seconds = int(remaining_batches * avg_time_per_batch)
                    if eta_seconds < 60:
                        eta_str = f"{eta_seconds}s"
                    else:
                        eta_str = f"{eta_seconds // 60}m {eta_seconds % 60}s"
                else:
                    eta_str = "è¨ˆç®—ä¸­..."

                progress_percent = 70 + int((total_processed / total_chunks) * 29)
                msg = f"ðŸ§  Embedding: {total_processed}/{total_chunks} ({int(total_processed / total_chunks * 100)}%) | å‰©é¤˜æ™‚é–“: {eta_str}"
                update_status(msg, progress_percent)
            except Exception as e:
                print(f"âš ï¸ Batch embedding å¤±æ•—: {e}")

    if latest_hash:
        os.makedirs(db_path, exist_ok=True)
        with open(hash_file, "w") as f: f.write(latest_hash)

    update_status("âœ… Ready!", 100)
    return db_path, "updated"